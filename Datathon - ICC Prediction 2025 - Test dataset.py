# -*- coding: utf-8 -*-
"""Datathon 2 2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vVivQqx5sl3ngJbhoGsO2ZU5Hz886c1j
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
file_path = "/mnt/data/Copy of Test Data_28 Feb 2025.xlsx"
df = pd.read_excel("/content/Copy of Test Data_28 Feb 2025.xlsx")

# Compute correlation matrix (only for numerical features)
correlation_matrix = df.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Team Performance Metrics")
plt.show()

# Return correlation matrix for review
correlation_matrix

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Define the path of the cleaned dataset
file_path = "/content/cleaned_dataset.csv"  # Change this to your actual file path

# Load the cleaned dataset
df = pd.read_csv(file_path)
print(f"Cleaned dataset '{file_path}' loaded successfully.\n")

# Display basic information
print("\nBasic Information:")
print(df.info())

# Display summary statistics
print("\nSummary Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Check for duplicate rows
print("\nDuplicate Rows:", df.duplicated().sum())

# Check column-wise unique values
print("\nColumn-wise Unique Values:")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

# Visualizing missing values
plt.figure(figsize=(10, 5))
sns.heatmap(df.isnull(), cmap='viridis', cbar=False)
plt.title("Missing Values Heatmap")
plt.show()

# Plot histograms for feature distributions
df.hist(figsize=(12, 10), bins=30, edgecolor='black')
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

import pandas as pd

# Create a DataFrame with past ICC Champions Trophy winners
historical_winners = pd.DataFrame({
    "Year": [1998, 2000, 2002, 2004, 2006, 2009, 2013, 2017],
    "Winner": ["South Africa", "New Zealand", "India/Sri Lanka", "West Indies",
               "Australia", "Australia", "India", "Pakistan"]
})

# Save to CSV (optional)
historical_winners.to_csv("/content/historical_winners.csv", index=False)
print("Historical winners dataset created!")

# Load your cleaned dataset
cleaned_dataset = pd.read_csv("/content/cleaned_dataset.csv")

# Ensure your cleaned dataset has a 'Year' column to match tournaments
if "Year" not in cleaned_dataset.columns:
    print("Your dataset must contain a 'Year' column to merge historical data!")

# Merge cleaned data with historical winners
merged_df = cleaned_dataset.merge(historical_winners, on="Year", how="left")

# Save the merged dataset for model training
merged_df.to_csv("/content/merged_dataset.csv", index=False)
print("Merged dataset saved successfully!")

print(merged_df.head())  # View sample rows
print(merged_df["Winner"].value_counts())  # Check winner distribution

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Step 1: Load the Cleaned Dataset
file_path = "/content/cleaned_dataset.csv"  # Update with actual file path
df = pd.read_csv(file_path)

# Step 2: Drop Non-Numeric Columns (If Any)
df_numeric = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Step 3: Apply Feature Scaling (Standardization)
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_numeric), columns=df_numeric.columns)

# Step 4: Compute VIF for Each Feature
vif_data = pd.DataFrame()
vif_data["Feature"] = df_scaled.columns
vif_data["VIF"] = [variance_inflation_factor(df_scaled.values, i) for i in range(df_scaled.shape[1])]

# Step 5: Display VIF Results
print("\nðŸ“Š Variance Inflation Factor (VIF) Analysis ðŸ“Š")
print(vif_data.sort_values(by="VIF", ascending=False))

# Step 6: Visualize Feature Correlation Matrix
plt.figure(figsize=(10, 6))
sns.heatmap(df_scaled.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Cleaned Dataset (Without Outcome Column)
file_path = "/content/cleaned_dataset.csv"  # Update with actual path
df = pd.read_csv(file_path)

# Step 2: Select Only Numeric Columns (Ensure No Categorical Variables)
df_numeric = df.select_dtypes(include=[np.number])

# Step 3: Apply Feature Scaling (Standardization)
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_numeric), columns=df_numeric.columns)

# Step 4: Apply PCA to Identify Feature Contribution
pca = PCA(n_components=2)  # Using 2 components for analysis
df_pca = pca.fit_transform(df_scaled)

# Step 5: Train a Random Forest Regressor to Compute Feature Importance
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(df_scaled, np.random.rand(df_scaled.shape[0]))  # Random target for unsupervised importance

# Step 6: Extract Feature Importances
feature_importance = pd.DataFrame({'Feature': df_numeric.columns, 'Importance': model.feature_importances_})
feature_importance = feature_importance.sort_values(by="Importance", ascending=False)

# Step 7: Plot Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance["Importance"], y=feature_importance["Feature"], palette="viridis")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Top Contributing Features (Unsupervised)")
plt.show()

# Step 8: Print the Top Features
print("\nðŸ“Š Top Contributing Features ðŸ“Š")
print(feature_importance.head(10))  # Display top 10 important features

import pandas as pd
import numpy as np

# Load the dataset
file_path = "/mnt/data/Copy of Test Data_28 Feb 2025.xlsx"
df = pd.read_excel("/content/Copy of Test Data_28 Feb 2025.xlsx")

# Display the first few rows to understand the structure
df.head()

# Monte Carlo Simulation for ICC Champions Trophy Prediction

# Define number of simulations
num_simulations = 10000

# Extract relevant features for probability estimation
teams = df["Team"].values
matches_won = df["Matches Won"].values
net_run_rate = df["NRR"].values

# Normalize probabilities (higher weight to matches won and NRR)
win_probabilities = (matches_won + (net_run_rate + abs(net_run_rate.min()) + 1)) / sum(matches_won + (net_run_rate + abs(net_run_rate.min()) + 1))

# Simulate tournaments
simulation_results = np.random.choice(teams, size=num_simulations, p=win_probabilities)

# Count how many times each team wins
team_win_counts = pd.Series(simulation_results).value_counts(normalize=True) * 100  # Convert to percentage

# Display results
team_win_counts.sort_values(ascending=False)

